---
ml_research:
  - name: "Gradient Descent"
    why: "Core algorithm for minimizing loss functions in training models."
    misconception: "People think it always finds the global minimum in non-convex problems."
    explanation_prompt: "Explain Gradient Descent in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Gradient_descent"
  - name: "Stochastic Gradient"
    why: "Enables scalable optimization using mini-batches for noisy but faster updates."
    misconception: "Assumed equivalent in convergence behavior to full-batch gradient descent."
    explanation_prompt: "Explain Stochastic Gradient in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
  - name: "Backpropagation"
    why: "Computes gradients efficiently through layered networks using the chain rule."
    misconception: "Confused as a separate optimizer rather than a gradient calculation method."
    explanation_prompt: "Explain Backpropagation in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Backpropagation"
  - name: "Activation Functions"
    why: "Introduce nonlinearity, enabling networks to approximate complex functions."
    misconception: "Assuming ReLU always outperforms sigmoid or tanh in every setting."
    explanation_prompt: "Explain Activation Functions in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Activation_function"
  - name: "Loss Functions"
    why: "Define what the model optimizes and encode task-specific goals."
    misconception: "Believing cross-entropy and MSE are interchangeable across tasks."
    explanation_prompt: "Explain Loss Functions in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Loss_function"
  - name: "Overfitting Control"
    why: "Prevents memorization, improving generalization to unseen data."
    misconception: "Thinking more data always fixes overfitting without regularization."
    explanation_prompt: "Explain Overfitting Control in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Overfitting"
  - name: "Bias–Variance Tradeoff"
    why: "Explains errors from model simplicity vs. complexity and guides tuning."
    misconception: "Assuming reducing bias never increases variance."
    explanation_prompt: "Explain Bias–Variance Tradeoff in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"
  - name: "Regularization (L2)"
    why: "Penalizes large weights to reduce variance and improve generalization."
    misconception: "Confusing L2 with early stopping or dropout effects."
    explanation_prompt: "Explain Regularization (L2) in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Tikhonov_regularization"
  - name: "Dropout Regularization"
    why: "Randomly disables units to prevent co-adaptation and overfitting."
    misconception: "Believing dropout should be used during inference."
    explanation_prompt: "Explain Dropout Regularization in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Dropout_(neural_networks)"
  - name: "Batch Normalization"
    why: "Stabilizes and accelerates training by normalizing activations."
    misconception: "Assuming it always allows much higher learning rates safely."
    explanation_prompt: "Explain Batch Normalization in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Batch_normalization"
  - name: "Learning Rate"
    why: "Controls step size in optimization and affects convergence stability."
    misconception: "Using a single constant value is optimal for all training phases."
    explanation_prompt: "Explain Learning Rate in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Learning_rate"
  - name: "Early Stopping"
    why: "Halts training before overfitting by monitoring validation performance."
    misconception: "Treating it as a substitute for explicit regularization."
    explanation_prompt: "Explain Early Stopping in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Early_stopping"
  - name: "Vanishing Gradients"
    why: "Diagnosis for unstable training in deep or recurrent networks."
    misconception: "Assuming it occurs only with sigmoid activations."
    explanation_prompt: "Explain Vanishing Gradients in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Vanishing_gradient_problem"
  - name: "Convolutional Networks"
    why: "Exploit spatial locality for images and signals via shared kernels."
    misconception: "Believing CNNs cannot process sequences or 1D signals."
    explanation_prompt: "Explain Convolutional Networks in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Convolutional_neural_network"
  - name: "Recurrent Networks"
    why: "Model temporal dependencies by maintaining hidden state over time."
    misconception: "Equating all RNNs with LSTMs in behavior and capability."
    explanation_prompt: "Explain Recurrent Networks in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Recurrent_neural_network"
  - name: "LSTM/GRU Units"
    why: "Use gating to mitigate vanishing gradients in sequence tasks."
    misconception: "Assuming LSTMs always outperform simpler RNNs regardless of data."
    explanation_prompt: "Explain LSTM/GRU Units in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Long_short-term_memory"
  - name: "Attention Mechanism"
    why: "Lets models focus on relevant parts of input when making predictions."
    misconception: "Thinking attention and interpretability are the same thing."
    explanation_prompt: "Explain Attention Mechanism in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Attention_(machine_learning)"
  - name: "Transformer Models"
    why: "State-of-the-art architecture for sequence modeling without recurrence."
    misconception: "Assuming transformers inherently require infinite data to train."
    explanation_prompt: "Explain Transformer Models in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
  - name: "Seq2Seq Models"
    why: "Map input sequences to output sequences across tasks like translation."
    misconception: "Confusing encoder–decoder with attention as mandatory."
    explanation_prompt: "Explain Seq2Seq Models in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Sequence-to-sequence"
  - name: "Word Embeddings"
    why: "Represent tokens in dense vectors capturing semantic relationships."
    misconception: "Believing embeddings are unique and directly interpretable dimensions."
    explanation_prompt: "Explain Word Embeddings in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Word_embedding"
  - name: "Transfer Learning"
    why: "Reuses knowledge from a source task to improve a target task."
    misconception: "Assuming naive fine-tuning never causes catastrophic forgetting."
    explanation_prompt: "Explain Transfer Learning in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Transfer_learning"
  - name: "Data Augmentation"
    why: "Expands training data variability to improve robustness."
    misconception: "Thinking augmentation can replace real data collection entirely."
    explanation_prompt: "Explain Data Augmentation in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Data_augmentation"
  - name: "Hyperparameter Tuning"
    why: "Systematically searches settings that optimize model performance."
    misconception: "Confusing hyperparameters with learned model parameters."
    explanation_prompt: "Explain Hyperparameter Tuning in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Hyperparameter_optimization"
  - name: "Cross-Validation"
    why: "Estimates generalization error using resampling schemes."
    misconception: "Treating test and validation folds as interchangeable."
    explanation_prompt: "Explain Cross-Validation in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
  - name: "Universal Approximation"
    why: "Shows neural nets can approximate continuous functions under conditions."
    misconception: "Misread as guaranteeing easy training or good generalization."
    explanation_prompt: "Explain Universal Approximation in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Universal_approximation_theorem"
ai_business:
  - name: "Total Cost of Ownership (TCO)"
    why: "Critical for budgeting AI projects beyond cloud bills."
    misconception: "Only counting compute costs and ignoring talent, maintenance, and opportunity costs."
    explanation_prompt: "Explain Total Cost of Ownership for AI projects in 2-3 sentences for business executives evaluating AI adoption. Focus on hidden costs and strategic planning."
    source: "https://en.wikipedia.org/wiki/Total_cost_of_ownership"
  - name: "Build vs Buy"
    why: "Strategic decision that impacts speed, cost, and competitive advantage."
    misconception: "Always building in-house for 'strategic control' without assessing opportunity cost."
    explanation_prompt: "Explain the Build vs Buy decision for AI capabilities in 2-3 sentences for business leaders. Focus on when each makes strategic sense."
    source: "https://en.wikipedia.org/wiki/Make_or_buy_decision"
  - name: "ROI Measurement"
    why: "Justifies AI investments and prioritizes initiatives."
    misconception: "Measuring only cost savings while ignoring revenue growth, risk reduction, and strategic value."
    explanation_prompt: "Explain how to measure ROI for AI projects in 2-3 sentences for executives. Focus on tangible and intangible benefits."
    source: "https://en.wikipedia.org/wiki/Return_on_investment"
  - name: "Vendor Lock-in"
    why: "Strategic risk from dependency on single AI providers."
    misconception: "Thinking open-source or APIs eliminate lock-in risks."
    explanation_prompt: "Explain vendor lock-in in AI/cloud services in 2-3 sentences for CTOs and procurement teams. Focus on mitigation strategies."
    source: "https://en.wikipedia.org/wiki/Vendor_lock-in"
  - name: "Technology Adoption Curve"
    why: "Guides go-to-market strategy and customer targeting."
    misconception: "Assuming all customers adopt at the same pace or for the same reasons."
    explanation_prompt: "Explain the Technology Adoption Curve for AI products in 2-3 sentences for product managers and marketers. Focus on segment characteristics."
    source: "https://en.wikipedia.org/wiki/Technology_adoption_life_cycle"
  - name: "Unit Economics"
    why: "Determines if AI business models are sustainable at scale."
    misconception: "Ignoring inference costs or assuming economies of scale will fix negative margins."
    explanation_prompt: "Explain unit economics for AI services in 2-3 sentences for founders and CFOs. Focus on per-user profitability."
    source: "https://en.wikipedia.org/wiki/Unit_economics"
  - name: "Competitive Moat"
    why: "Protects AI businesses from commoditization."
    misconception: "Believing model performance alone creates defensibility."
    explanation_prompt: "Explain competitive moats in AI businesses in 2-3 sentences for investors and strategists. Focus on durable advantages."
    source: "https://en.wikipedia.org/wiki/Economic_moat"
  - name: "Go-to-Market Strategy"
    why: "Determines how AI products reach and win customers."
    misconception: "Copying consumer PLG playbooks for enterprise AI sales."
    explanation_prompt: "Explain go-to-market strategies for AI products in 2-3 sentences for founders and sales leaders. Focus on customer acquisition channels."
    source: "https://en.wikipedia.org/wiki/Go_to_market"
  - name: "Customer Acquisition Cost (CAC)"
    why: "Measures efficiency of sales and marketing spend."
    misconception: "Optimizing CAC without considering customer lifetime value or payback period."
    explanation_prompt: "Explain Customer Acquisition Cost for AI SaaS in 2-3 sentences for marketing and finance leaders. Focus on benchmarks and optimization."
    source: "https://en.wikipedia.org/wiki/Customer_acquisition_cost"
  - name: "Lifetime Value (LTV)"
    why: "Quantifies long-term revenue from customers."
    misconception: "Assuming historical retention rates will hold for AI products with rapidly evolving alternatives."
    explanation_prompt: "Explain Lifetime Value for AI subscription businesses in 2-3 sentences for growth teams. Focus on retention and expansion."
    source: "https://en.wikipedia.org/wiki/Customer_lifetime_value"
  - name: "Churn Rate"
    why: "Critical metric for SaaS and subscription AI services."
    misconception: "Only tracking logo churn while ignoring revenue churn and expansion."
    explanation_prompt: "Explain churn rate for AI SaaS products in 2-3 sentences for customer success leaders. Focus on leading indicators."
    source: "https://en.wikipedia.org/wiki/Churn_rate"
  - name: "Product-Market Fit"
    why: "Determines if AI product solves real customer pain."
    misconception: "Confusing enthusiasm from early adopters with broad market demand."
    explanation_prompt: "Explain product-market fit for AI products in 2-3 sentences for founders and product leaders. Focus on measurement signals."
    source: "https://en.wikipedia.org/wiki/Product/market_fit"
  - name: "Network Effects"
    why: "Creates exponential value growth as user base expands."
    misconception: "Claiming network effects exist when product value doesn't actually increase with users."
    explanation_prompt: "Explain network effects in AI platforms in 2-3 sentences for strategists. Focus on types and sustainability."
    source: "https://en.wikipedia.org/wiki/Network_effect"
  - name: "Pricing Strategy"
    why: "Captures value and positions product in market."
    misconception: "Pricing based on costs rather than customer value or competitive positioning."
    explanation_prompt: "Explain pricing strategies for AI products in 2-3 sentences for product and revenue leaders. Focus on value-based pricing."
    source: "https://en.wikipedia.org/wiki/Pricing_strategies"
  - name: "Minimum Viable Product (MVP)"
    why: "Tests market demand with minimal investment."
    misconception: "Building 'minimum' as 'cheap and buggy' rather than 'smallest thing that validates hypothesis'."
    explanation_prompt: "Explain MVP for AI products in 2-3 sentences for product managers. Focus on learning objectives vs feature scope."
    source: "https://en.wikipedia.org/wiki/Minimum_viable_product"
  - name: "A/B Testing"
    why: "Measures causal impact of product changes on business metrics."
    misconception: "Stopping tests early when results look good or running tests without statistical power."
    explanation_prompt: "Explain A/B testing for AI product decisions in 2-3 sentences for growth and product teams. Focus on statistical rigor."
    source: "https://en.wikipedia.org/wiki/A/B_testing"
  - name: "North Star Metric"
    why: "Aligns entire organization around key value driver."
    misconception: "Picking vanity metrics like signups instead of metrics tied to customer value."
    explanation_prompt: "Explain North Star Metrics for AI companies in 2-3 sentences for executives. Focus on selection criteria."
    source: "https://en.wikipedia.org/wiki/North_star_metric"
  - name: "Burn Rate"
    why: "Determines runway and urgency for fundraising or profitability."
    misconception: "Ignoring burn rate during growth phases or assuming growth always justifies losses."
    explanation_prompt: "Explain burn rate for AI startups in 2-3 sentences for founders and CFOs. Focus on sustainable growth."
    source: "https://en.wikipedia.org/wiki/Burn_rate"
  - name: "Runway"
    why: "Measures time until company runs out of cash."
    misconception: "Calculating runway without accounting for revenue growth or variable costs."
    explanation_prompt: "Explain financial runway for AI companies in 2-3 sentences for finance leaders. Focus on scenario planning."
    source: "https://en.wikipedia.org/wiki/Burn_rate"
  - name: "Term Sheet Basics"
    why: "Defines key terms in fundraising that impact control and economics."
    misconception: "Focusing only on valuation while ignoring liquidation preferences, board seats, and protective provisions."
    explanation_prompt: "Explain term sheets for AI startup fundraising in 2-3 sentences for founders. Focus on key terms beyond valuation."
    source: "https://en.wikipedia.org/wiki/Term_sheet"
  - name: "Dilution"
    why: "Impacts founder and employee ownership over funding rounds."
    misconception: "Viewing dilution as purely negative rather than evaluating value created per dollar raised."
    explanation_prompt: "Explain equity dilution in 2-3 sentences for founders raising capital. Focus on dilution vs value creation tradeoff."
    source: "https://en.wikipedia.org/wiki/Stock_dilution"
  - name: "Regulatory Compliance"
    why: "Required for legal AI operation in different jurisdictions."
    misconception: "Treating compliance as one-time launch checklist rather than ongoing process."
    explanation_prompt: "Explain AI regulatory compliance (GDPR, AI Act) in 2-3 sentences for legal and compliance teams. Focus on key obligations."
    source: "https://en.wikipedia.org/wiki/Regulatory_compliance"
  - name: "Data Privacy Laws"
    why: "Governs how AI companies can collect and use customer data."
    misconception: "Believing privacy policies and consent checkboxes are sufficient for compliance."
    explanation_prompt: "Explain data privacy laws (GDPR, CCPA) for AI businesses in 2-3 sentences for legal teams. Focus on rights and obligations."
    source: "https://en.wikipedia.org/wiki/General_Data_Protection_Regulation"
  - name: "Intellectual Property (IP)"
    why: "Protects innovations and creates business value."
    misconception: "Assuming patents are always valuable or that trade secrets are always better."
    explanation_prompt: "Explain IP strategy for AI companies in 2-3 sentences for founders and legal teams. Focus on patents vs trade secrets."
    source: "https://en.wikipedia.org/wiki/Intellectual_property"
  - name: "Platform Risk"
    why: "Dependency on external platforms that can change rules."
    misconception: "Building entire business on platform APIs without diversification plan."
    explanation_prompt: "Explain platform risk for AI businesses in 2-3 sentences for strategists. Focus on mitigation strategies."
    source: "https://en.wikipedia.org/wiki/Platform_risk"
ai_ethics:
  - name: "Incident Response"
    why: "Coordinates rapid mitigation of outages or harmful model behavior."
    misconception: "Confusing postmortems with blame assignments."
    explanation_prompt: "Explain Incident Response in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Incident_response"
  - name: "Algorithmic Bias"
    why: "Systematic errors can disadvantage groups in model outcomes."
    misconception: "Assuming balanced datasets automatically remove bias."
    explanation_prompt: "Explain Algorithmic Bias in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Algorithmic_bias"
  - name: "Fairness Metrics"
    why: "Provide formal criteria to assess equitable model performance."
    misconception: "Thinking all fairness metrics can be satisfied simultaneously."
    explanation_prompt: "Explain Fairness Metrics in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Fairness_(machine_learning)"
  - name: "Equalized Odds"
    why: "Requires equal error rates across protected groups."
    misconception: "Confusing it with demographic parity on outcomes."
    explanation_prompt: "Explain Equalized Odds in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Equalized_odds"
  - name: "Demographic Parity"
    why: "Demands equal positive rates regardless of group membership."
    misconception: "Mistaking it for equal accuracy across groups."
    explanation_prompt: "Explain Demographic Parity in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Demographic_parity"
  - name: "Disparate Impact"
    why: "Flags policies with adverse effects despite neutral intent."
    misconception: "Believing intent determines discrimination legality."
    explanation_prompt: "Explain Disparate Impact in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Disparate_impact"
  - name: "Informed Consent"
    why: "Ensures individuals understand and agree to data use."
    misconception: "Treating vague opt-ins as fully informed consent."
    explanation_prompt: "Explain Informed Consent in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Informed_consent"
  - name: "Data Minimization"
    why: "Limits collection to necessary data, reducing risk exposure."
    misconception: "Assuming minimization conflicts with performance by default."
    explanation_prompt: "Explain Data Minimization in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Data_minimization"
  - name: "Differential Privacy"
    why: "Protects individual contributions with formal privacy guarantees."
    misconception: "Believing epsilon has an absolute universal 'safe' value."
    explanation_prompt: "Explain Differential Privacy in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Differential_privacy"
  - name: "K-Anonymity"
    why: "Prevents re-identification by ensuring records are indistinguishable."
    misconception: "Thinking k-anonymity alone prevents all linkage attacks."
    explanation_prompt: "Explain K-Anonymity in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/K-anonymity"
  - name: "Federated Learning"
    why: "Trains models across devices without centralizing raw data."
    misconception: "Assuming it inherently guarantees privacy without noise."
    explanation_prompt: "Explain Federated Learning in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Federated_learning"
  - name: "Explainable AI"
    why: "Provides human-understandable reasons for model behavior."
    misconception: "Equating post-hoc explanations with ground truth causality."
    explanation_prompt: "Explain Explainable AI in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence"
  - name: "Interpretability"
    why: "Assesses how well humans can comprehend model mechanisms."
    misconception: "Assuming interpretability and accuracy are mutually exclusive."
    explanation_prompt: "Explain Interpretability in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Interpretability"
  - name: "Transparency"
    why: "Discloses information about data, models, and processes."
    misconception: "Thinking transparency alone ensures fairness or safety."
    explanation_prompt: "Explain Transparency in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Transparency_(behavior)"
  - name: "Accountability"
    why: "Assigns responsibility for model decisions and outcomes."
    misconception: "Assuming accountability ends with technical teams."
    explanation_prompt: "Explain Accountability in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Accountability"
  - name: "AI Safety"
    why: "Studies preventing unintended harmful behavior in AI systems."
    misconception: "Equating safety solely with adversarial robustness."
    explanation_prompt: "Explain AI Safety in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/AI_safety"
  - name: "Adversarial ML"
    why: "Examines vulnerabilities from crafted inputs that fool models."
    misconception: "Assuming training accuracy implies adversarial robustness."
    explanation_prompt: "Explain Adversarial ML in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Adversarial_machine_learning"
  - name: "Robustness"
    why: "Measures performance stability under distributional shifts or noise."
    misconception: "Confusing robustness with simple regularization strength."
    explanation_prompt: "Explain Robustness in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Robustness_(computer_science)"
  - name: "Surveillance Risks"
    why: "Relates to mass data collection and potential rights violations."
    misconception: "Assuming public data use is always ethically permissible."
    explanation_prompt: "Explain Surveillance Risks in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Surveillance"
  - name: "Facial Recognition"
    why: "Raises issues of accuracy disparities and pervasive tracking."
    misconception: "Believing accuracy on one dataset generalizes to all groups."
    explanation_prompt: "Explain Facial Recognition in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Facial_recognition_system"
  - name: "Data Governance"
    why: "Sets policies and processes for responsible data lifecycle management."
    misconception: "Equating governance with access denial rather than stewardship."
    explanation_prompt: "Explain Data Governance in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Data_governance"
  - name: "GDPR Basics"
    why: "Core EU privacy law shaping consent, rights, and processing."
    misconception: "Assuming GDPR applies only to EU-based companies."
    explanation_prompt: "Explain GDPR Basics in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/General_Data_Protection_Regulation"
  - name: "CCPA Overview"
    why: "California law granting consumer data rights impacting products."
    misconception: "Confusing CCPA with GDPR scope and definitions."
    explanation_prompt: "Explain CCPA Overview in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act"
  - name: "Value Sensitive Design"
    why: "Integrates human values into system design systematically."
    misconception: "Assuming values can be added post-deployment easily."
    explanation_prompt: "Explain Value Sensitive Design in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Value_sensitive_design"
  - name: "Human Oversight"
    why: "Ensures meaningful review and override of automated decisions."
    misconception: "Assuming oversight is effective without feedback channels."
    explanation_prompt: "Explain Human Oversight in 2-3 sentences for ML engineers who learned it in school but forgot the details. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Human-in-the-loop"
data_science:
  - name: "Content Moderation"
    why: "Balances safety, speech, and fairness in automated systems."
    misconception: "Believing automated moderation is fully objective."
    explanation_prompt: "Explain Content Moderation in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Content_moderation"
  - name: "Probability Basics"
    why: "Foundation for modeling uncertainty and making predictions."
    misconception: "Confusing odds with probabilities and frequencies."
    explanation_prompt: "Explain Probability Basics in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Probability"
  - name: "Bayes Theorem"
    why: "Updates beliefs with evidence, central to probabilistic inference."
    misconception: "Treating priors as arbitrary and inconsequential."
    explanation_prompt: "Explain Bayes Theorem in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on prior/posterior distributions, statistical inference, and hypothesis testing. NO machine learning examples - focus on traditional statistics."
    source: "https://en.wikipedia.org/wiki/Bayes%27_theorem"
  - name: "Sampling Methods"
    why: "Determine representativeness and variance of estimates."
    misconception: "Assuming larger samples always fix sampling bias."
    explanation_prompt: "Explain Sampling Methods in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Sampling_(statistics)"
  - name: "Central Limit Theorem"
    why: "Justifies normal approximations for sums and means."
    misconception: "Believing it applies regardless of sample size or dependence."
    explanation_prompt: "Explain Central Limit Theorem in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Central_limit_theorem"
  - name: "Hypothesis Testing"
    why: "Framework for deciding significance of observed effects."
    misconception: "Interpreting p-values as the probability the null is true."
    explanation_prompt: "Explain Hypothesis Testing in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"
  - name: "P-Values"
    why: "Quantify compatibility of data with a specified null model."
    misconception: "Treating 0.05 as a magic threshold for truth."
    explanation_prompt: "Explain P-Values in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/P-value"
  - name: "Confidence Intervals"
    why: "Provide ranges for parameter estimates with specified coverage."
    misconception: "Interpreting them as direct probabilities for parameters."
    explanation_prompt: "Explain Confidence Intervals in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Confidence_interval"
  - name: "Statistical Power"
    why: "Controls false negatives and sample size planning."
    misconception: "Confusing power with significance level."
    explanation_prompt: "Explain Statistical Power in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Statistical_power"
  - name: "Correlation vs Causation"
    why: "Prevents misinterpretation of associations as causal effects."
    misconception: "Assuming high correlation implies causality."
    explanation_prompt: "Explain Correlation vs Causation in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation"
  - name: "Linear Regression"
    why: "Baseline model for prediction and inference with continuous outcomes."
    misconception: "Ignoring assumptions like linearity and homoscedasticity."
    explanation_prompt: "Explain Linear Regression in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Linear_regression"
  - name: "Logistic Regression"
    why: "Standard classifier for binary outcomes with probabilistic outputs."
    misconception: "Treating predicted probabilities as calibrated by default."
    explanation_prompt: "Explain Logistic Regression in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Logistic_regression"
  - name: "Regularization L1/L2"
    why: "Controls model complexity and improves generalization."
    misconception: "Assuming penalties do not bias parameter estimates."
    explanation_prompt: "Explain Regularization L1/L2 in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Regularization_(mathematics)"
  - name: "Feature Selection"
    why: "Reduces overfitting and improves interpretability and speed."
    misconception: "Selecting features solely by univariate correlation."
    explanation_prompt: "Explain Feature Selection in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Feature_selection"
  - name: "Principal Components"
    why: "Transforms correlated features into orthogonal components."
    misconception: "Interpreting PCs as the original features with weights."
    explanation_prompt: "Explain Principal Components in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Principal_component_analysis"
  - name: "Time Series Basics"
    why: "Addresses autocorrelation, trend, and seasonality in temporal data."
    misconception: "Applying IID assumptions directly to time series."
    explanation_prompt: "Explain Time Series Basics in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Time_series"
  - name: "Stationarity Checks"
    why: "Determines valid modeling choices and differencing needs."
    misconception: "Confusing strict, weak, and trend stationarity."
    explanation_prompt: "Explain Stationarity Checks in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Stationary_process"
  - name: "ARIMA Modeling"
    why: "Classical approach for univariate time series forecasting."
    misconception: "Treating ARIMA as a black box without diagnostics."
    explanation_prompt: "Explain ARIMA Modeling in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average"
  - name: "Missing Data"
    why: "Handling mechanisms (MCAR/MAR/MNAR) affects inference validity."
    misconception: "Assuming mean imputation is harmless."
    explanation_prompt: "Explain Missing Data in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Missing_data"
  - name: "Outlier Treatment"
    why: "Prevents extreme values from distorting models and metrics."
    misconception: "Removing outliers solely by z-score without context."
    explanation_prompt: "Explain Outlier Treatment in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Outlier"
  - name: "Normalization Scaling"
    why: "Ensures features are on comparable scales for many algorithms."
    misconception: "Confusing normalization with standardization."
    explanation_prompt: "Explain Normalization Scaling in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Feature_scaling"
  - name: "EDA Principles"
    why: "Reveals structure, anomalies, and hypotheses before modeling."
    misconception: "Skipping EDA when using automated pipelines."
    explanation_prompt: "Explain EDA Principles in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Exploratory_data_analysis"
  - name: "AB Test Design"
    why: "Plans experiments with randomization, control, and power."
    misconception: "Peeking repeatedly without correction."
    explanation_prompt: "Explain AB Test Design in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Design_of_experiments"
  - name: "Cross-Validation"
    why: "Estimates model performance reliably on limited data."
    misconception: "Using test data inside cross-validation loops."
    explanation_prompt: "Explain Cross-Validation in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
  - name: "SQL Joins"
    why: "Combine tables correctly to build analysis datasets."
    misconception: "Confusing inner joins with left joins and causing row loss."
    explanation_prompt: "Explain SQL Joins in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Join_(SQL)"
  - name: "SQL Windows"
    why: "Compute analytics over partitions without collapsing rows."
    misconception: "Assuming GROUP BY and window functions are interchangeable."
    explanation_prompt: "Explain SQL Windows in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Window_function_(SQL)"
  - name: "Data Visualization"
    why: "Communicates findings clearly through appropriate encodings."
    misconception: "Using 3D or dual axes when simple charts suffice."
    explanation_prompt: "Explain Data Visualization in 2-3 sentences for data scientists and analysts who need a statistical foundation. Focus on intuition and when to use it."
    source: "https://en.wikipedia.org/wiki/Data_and_information_visualization"

